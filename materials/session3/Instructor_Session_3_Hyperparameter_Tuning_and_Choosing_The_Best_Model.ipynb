{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Downlaoding/Uploading Data"
      ],
      "metadata": {
        "id": "t9U3SF3itBbS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiAacJC9stny"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ciol-researchlab/CIOL-Winter-ML-Bootcamp.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setting up the enviroment"
      ],
      "metadata": {
        "id": "4FWMiUUKtEUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tabjular Data Analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Utility\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "5q_2mc-2tFmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "ZLREaEEYtHdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/CIOL-Winter-ML-Bootcamp/datasets/session3/main/st/train_cleaned.csv')\n",
        "test_df = pd.read_csv('/content/CIOL-Winter-ML-Bootcamp/datasets/session3/main/st/test_cleaned.csv')\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "cQqSXS0ZtLEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly select 500 samples from train_df\n",
        "train_df = train_df.sample(n=500, random_state=42)\n",
        "\n",
        "# Randomly select 100 samples from test_df\n",
        "test_df = test_df.sample(n=100, random_state=42)"
      ],
      "metadata": {
        "id": "Z9h4YexQtbHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df.drop('Transported', axis =1 )\n",
        "y = train_df['Transported']"
      ],
      "metadata": {
        "id": "0ydOiFofvMQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Cross Validation"
      ],
      "metadata": {
        "id": "L_-l2Ye6uUgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation is a technique used in machine learning to evaluate a model's performance and ensure that it generalizes well to unseen data. It involves splitting the dataset into multiple parts, training the model on some parts, and testing it on others. This helps reduce overfitting and provides a more reliable estimate of how the model will perform in real-world scenarios.\n",
        "\n",
        "\n",
        "- [ML Foundation ➡️ Cross Validation ✅ All Methods](https://www.kaggle.com/code/azminetoushikwasi/ml-foundation-cross-validation-all-methods)\n",
        "- [Scikit-learn | Model Selection](https://scikit-learn.org/stable/api/sklearn.model_selection.html)"
      ],
      "metadata": {
        "id": "3lxliGMvucNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "nfxxCnpttuZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(random_state=42)"
      ],
      "metadata": {
        "id": "CuxY7F56FDaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
        "\n",
        "# Print scores of all folds\n",
        "print(\"Scores of all folds:\", scores)\n",
        "print(\"Mean Accuracy:\", np.mean(scores))"
      ],
      "metadata": {
        "id": "AE3SLVNizTSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 5-fold cross-validation\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
        "\n",
        "# Print scores of all folds\n",
        "print(\"Scores of all folds:\", scores)\n",
        "print(\"Mean Accuracy:\", np.mean(scores))"
      ],
      "metadata": {
        "id": "azpaMPhW_2IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# Define multiple scoring metrics\n",
        "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "# Perform cross-validation with multiple scoring metrics\n",
        "cv_results = cross_validate(model, X, y, cv=5, scoring=scoring, return_train_score=True)\n",
        "\n",
        "# Print results for each metric\n",
        "for metric in scoring:\n",
        "    print(f\"{metric.capitalize()} scores (test): {cv_results[f'test_{metric}']}\")\n",
        "    print(f\"Mean {metric.capitalize()} (test): {cv_results[f'test_{metric}'].mean():.4f}\\n\")\n",
        "\n",
        "# Print train/test scores for accuracy\n",
        "print(\"Train Accuracy Scores:\", cv_results['train_accuracy'])\n",
        "print(\"Test Accuracy Scores:\", cv_results['test_accuracy'])"
      ],
      "metadata": {
        "id": "XSNwQyha_sg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bias-Variance Tradeoff"
      ],
      "metadata": {
        "id": "BLdp7p_9hbpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Create a binary matrix to indicate fold assignments\n",
        "fold_matrix = np.zeros((500, 5))\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
        "    fold_matrix[test_index, fold] = 1  # Mark test samples as 1\n",
        "\n",
        "# Plot the fold assignment matrix\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(fold_matrix, cmap=\"coolwarm\", aspect=\"auto\")\n",
        "plt.colorbar(label=\"Fold Assignment (0=Train, 1=Test)\")\n",
        "plt.xlabel(\"Fold\")\n",
        "plt.ylabel(\"Sample Index\")\n",
        "plt.title(\"Visualization of Fold Assignments\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cs_ImFkrzWC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "dFkNz4ReAP36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning is the process of selecting the best configuration of hyperparameters for a machine learning model to maximize its performance. Unlike model parameters (which are learned during training), hyperparameters are external configurations set before training begins.\n",
        "\n",
        "---\n",
        "\n",
        "**What are Hyperparameters?**\n",
        "- **Model-Specific Hyperparameters**:\n",
        "  - Examples:\n",
        "    - Number of trees in a Random Forest (`n_estimators`).\n",
        "    - Learning rate in Gradient Boosting.\n",
        "    - Number of layers in a Neural Network.\n",
        "- **Training-Specific Hyperparameters**:\n",
        "  - Examples:\n",
        "    - Batch size.\n",
        "    - Number of epochs.\n",
        "    - Optimizer type (e.g., Adam, SGD).\n",
        "\n",
        "---\n",
        "\n",
        "**Why is Hyperparameter Tuning Important?**\n",
        "The choice of hyperparameters can significantly affect a model's performance. Proper tuning helps:\n",
        "- Avoid overfitting or underfitting.\n",
        "- Improve generalization to unseen data.\n",
        "- Achieve better accuracy, precision, recall, or other metrics.\n",
        "\n",
        "**Automated Tools**:\n",
        "   - Libraries like **Optuna**, **Hyperopt**, and **Ray Tune** automate hyperparameter tuning.\n",
        "\n",
        "---\n",
        "\n",
        "**Steps in Hyperparameter Tuning**\n",
        "1. **Define the Search Space**:\n",
        "   - Decide which hyperparameters to tune and their possible values or ranges.\n",
        "\n",
        "2. **Select a Search Strategy**:\n",
        "   - Choose between grid search, random search, or advanced methods.\n",
        "\n",
        "3. **Cross-Validation**:\n",
        "   - Use k-fold cross-validation to evaluate model performance for each hyperparameter combination.\n",
        "\n",
        "4. **Evaluate and Choose the Best**:\n",
        "   - Compare models based on metrics (e.g., accuracy, precision).\n",
        "   - Choose the configuration that provides the best validation score.\n"
      ],
      "metadata": {
        "id": "-2M7-6OrAapZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Manual Search**:\n",
        "   - Trial-and-error approach based on experience.\n",
        "   - Simple but not scalable for complex models."
      ],
      "metadata": {
        "id": "41aY-COEBa1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Manual hyperparameter tuning\n",
        "params = [\n",
        "    {'n_estimators': 50, 'max_depth': 5},\n",
        "    {'n_estimators': 100, 'max_depth': 10},\n",
        "    {'n_estimators': 150, 'max_depth': None},\n",
        "]\n",
        "\n",
        "best_score = 0\n",
        "best_params = {}\n",
        "\n",
        "for param in params:\n",
        "    model = RandomForestClassifier(**param, random_state=42)\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "    mean_score = scores.mean()\n",
        "    print(f\"Params: {param}, Accuracy: {mean_score:.4f}\")\n",
        "    if mean_score > best_score:\n",
        "        best_score = mean_score\n",
        "        best_params = param\n",
        "\n",
        "print(\"\\nBest Parameters (Manual Search):\", best_params)"
      ],
      "metadata": {
        "id": "FL_XFNAxBVZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Grid Search**:\n",
        "   - Exhaustively tries all combinations of specified hyperparameter values.\n",
        "   - Computationally expensive for large search spaces."
      ],
      "metadata": {
        "id": "LZgrNQ5FBeza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        ")\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(\"\\nBest Parameters (Grid Search):\", grid_search.best_params_)\n",
        "print(\"Best Accuracy (Grid Search):\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "5xRLgVCRBuLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search_res=pd.DataFrame(grid_search.cv_results_)\n",
        "grid_search_res"
      ],
      "metadata": {
        "id": "dK4HCr3QCL3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Random Search**:\n",
        "   - Randomly samples hyperparameter combinations.\n",
        "   - Faster than grid search for large spaces, especially when only a subset of hyperparameters has a significant impact."
      ],
      "metadata": {
        "id": "qZNfE-X3BiR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.arange(50, 201, 10)"
      ],
      "metadata": {
        "id": "tt_JEergmQ-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# Define hyperparameter distribution\n",
        "param_dist = {\n",
        "    'n_estimators': np.arange(50, 201, 10),\n",
        "    'max_depth': [5, 10, None],\n",
        "    'min_samples_split': np.arange(2, 21),\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1,\n",
        ")\n",
        "random_search.fit(X, y)\n",
        "\n",
        "print(\"\\nBest Parameters (Random Search):\", random_search.best_params_)\n",
        "print(\"Best Accuracy (Random Search):\", random_search.best_score_)\n"
      ],
      "metadata": {
        "id": "wUTCcF47B0Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bayesian Optimization**:\n",
        "   - Models the relationship between hyperparameters and performance.\n",
        "   - Efficiently finds optimal parameters using probabilistic methods."
      ],
      "metadata": {
        "id": "vVKbF9auBjyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "srUw0bNRCIpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Define the objective function for optimization\n",
        "def objective(trial):\n",
        "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
        "    max_depth = trial.suggest_int('max_depth', 5, 30)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
        "\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        random_state=42,\n",
        "    )\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "# Create a study and optimize\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(\"\\nBest Parameters (Bayesian Optimization):\", study.best_params)\n",
        "print(\"Best Accuracy (Bayesian Optimization):\", study.best_value)\n"
      ],
      "metadata": {
        "id": "xyB7EOO8Bg15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality Reduction\n",
        "\n",
        "Dimensionality reduction is the process of reducing the number of input variables (features) in a dataset while retaining as much relevant information as possible. It simplifies data analysis and visualization, especially in high-dimensional datasets, by projecting data into a lower-dimensional space.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why is Dimensionality Reduction Done?**\n",
        "1. **Avoid Overfitting**:\n",
        "   - High-dimensional data often suffers from overfitting due to the curse of dimensionality.\n",
        "\n",
        "2. **Improve Performance**:\n",
        "   - Reducing dimensions can speed up training for machine learning models and reduce computational cost.\n",
        "\n",
        "3. **Visualization**:\n",
        "   - High-dimensional data is difficult to visualize. Dimensionality reduction techniques like t-SNE and UMAP can project data into 2D or 3D for better understanding.\n",
        "\n",
        "4. **Noise Removal**:\n",
        "   - Helps remove irrelevant or redundant features that can add noise to the model.\n",
        "\n",
        "5. **Reduce Storage**:\n",
        "   - A compact representation of data can save storage and improve efficiency.\n",
        "\n",
        "\n",
        "**Curse of dimensionality** refers to the challenges faced when working with high-dimensional data. As the number of features increases, the volume of the space grows exponentially, requiring more data to maintain statistical significance. Distance metrics become less meaningful, leading to difficulties in clustering and classification. This sparsity can cause models to overfit and generalize poorly. Additionally, computational costs and the complexity of visualizing high-dimensional data increase. Mitigating the curse involves techniques like dimensionality reduction (e.g., PCA, t-SNE), feature selection, and regularization to simplify the data and improve model performance."
      ],
      "metadata": {
        "id": "Od3EP5f1C17y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Principal Component Analysis (PCA)**\n",
        "- **What it Does**:\n",
        "  - Projects data into a new coordinate system where the axes (principal components) capture the maximum variance in the data.\n",
        "  - Retains only the top `k` principal components to reduce dimensions.\n",
        "\n",
        "- **How it Works**:\n",
        "  1. Compute the covariance matrix of the data.\n",
        "  2. Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
        "  3. Sort eigenvectors by eigenvalues (largest to smallest) to rank the principal components.\n",
        "  4. Project data onto the top `k` eigenvectors.\n",
        "\n",
        "- **Advantages**:\n",
        "  - Preserves global structure.\n",
        "  - Linear method: fast and interpretable.\n",
        "\n",
        "- **Limitations**:\n",
        "  - Assumes linear relationships.\n",
        "  - Sensitive to outliers and scaling."
      ],
      "metadata": {
        "id": "0T3jye9IDCrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "X_pca = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "VLuXfHU7DdnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "mBSX2tlUDumu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_pca.shape"
      ],
      "metadata": {
        "id": "CC_o61LkDwkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n",
        "- **What it Does**:\n",
        "  - Maps high-dimensional data into a lower-dimensional space (often 2D or 3D) while preserving the local structure of the data (i.e., clusters).\n",
        "\n",
        "- **How it Works**:\n",
        "  1. Converts distances between points in high-dimensional space into probabilities (similarity).\n",
        "  2. Does the same for the lower-dimensional space.\n",
        "  3. Minimizes the Kullback-Leibler (KL) divergence between the two probability distributions.\n",
        "\n",
        "- **Advantages**:\n",
        "  - Excellent for visualizing clusters and local structure.\n",
        "  - Non-linear and handles complex manifolds.\n",
        "\n",
        "- **Limitations**:\n",
        "  - Computationally expensive for large datasets.\n",
        "  - Cannot preserve global structure.\n",
        "  - Results are non-deterministic without setting a random seed.\n"
      ],
      "metadata": {
        "id": "3B5Ehr4xDF8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "X_tsne = tsne.fit_transform(X)"
      ],
      "metadata": {
        "id": "YB_e6bnuDhF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tsne.shape"
      ],
      "metadata": {
        "id": "4oYWwvFaD4rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a figure\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
        "axes[0].set_title('PCA')\n",
        "axes[0].set_xlabel('Principal Component 1')\n",
        "axes[0].set_ylabel('Principal Component 2')\n",
        "\n",
        "axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
        "axes[1].set_title('t-SNE')\n",
        "axes[1].set_xlabel('t-SNE Component 1')\n",
        "axes[1].set_ylabel('t-SNE Component 2')\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WXUW-P7iD0po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Uniform Manifold Approximation and Projection (UMAP)**\n",
        "- **What it Does**:\n",
        "  - Similar to t-SNE but focuses on both local and global structure. Faster and often better for clustering tasks.\n",
        "\n",
        "- **How it Works**:\n",
        "  1. Constructs a high-dimensional graph based on distances between points.\n",
        "  2. Optimizes a low-dimensional graph to maintain relationships between points.\n",
        "  3. Uses stochastic gradient descent to map high-dimensional points to a low-dimensional space.\n",
        "\n",
        "- **Advantages**:\n",
        "  - Faster and more scalable than t-SNE.\n",
        "  - Preserves both local and global structure.\n",
        "  - Deterministic if initialized correctly.\n",
        "\n",
        "- **Limitations**:\n",
        "  - Sensitive to hyperparameters (e.g., `n_neighbors` and `min_dist`).\n"
      ],
      "metadata": {
        "id": "TxzLL0WWDJ_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "X8rJNCuZEEKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "\n",
        "# Initialize UMAP\n",
        "umap_model = umap.UMAP(n_components=2, random_state=42)\n",
        "\n",
        "# Apply UMAP to reduce dimensions\n",
        "X_umap = umap_model.fit_transform(X)"
      ],
      "metadata": {
        "id": "jW86gQuUEJxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_umap.shape"
      ],
      "metadata": {
        "id": "GHMniNzErfsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the results\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
        "plt.title('UMAP Dimension Reduction')\n",
        "plt.xlabel('UMAP Component 1')\n",
        "plt.ylabel('UMAP Component 2')\n",
        "plt.colorbar(label='Class Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ihiZv1pWELmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison\n",
        "\n",
        "**Comparison of PCA, t-SNE, and UMAP**\n",
        "\n",
        "| Feature                | PCA                  | t-SNE                  | UMAP                  |\n",
        "|------------------------|----------------------|------------------------|-----------------------|\n",
        "| **Type**              | Linear               | Non-linear             | Non-linear            |\n",
        "| **Speed**             | Fast                 | Slow                   | Faster than t-SNE     |\n",
        "| **Scalability**       | High                 | Medium                 | High                  |\n",
        "| **Global Structure**  | Preserved            | Not preserved          | Partially preserved   |\n",
        "| **Local Structure**   | Weakly preserved     | Strongly preserved     | Strongly preserved    |\n",
        "| **Use Case**          | Data compression, preprocessing | Visualization | Visualization, clustering |\n",
        "\n",
        "---\n",
        "\n",
        "**How to Choose the Right Algorithm**\n",
        "- **PCA**:\n",
        "  - Use when global structure is important, and relationships are linear.\n",
        "  - Ideal for preprocessing data for machine learning.\n",
        "\n",
        "- **t-SNE**:\n",
        "  - Best for exploring and visualizing clusters in small to medium-sized datasets.\n",
        "\n",
        "- **UMAP**:\n",
        "  - Preferable for large datasets or when clustering and preserving both local and global structures are essential.\n"
      ],
      "metadata": {
        "id": "PKK-DZqwDNeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Networks"
      ],
      "metadata": {
        "id": "UuHNTR0xF0Vv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is an Artificial Neural Network (ANN)?**\n",
        "\n",
        "An **Artificial Neural Network (ANN)** is a computational model inspired by the way biological neural networks in the human brain work. It consists of layers of interconnected nodes (neurons), where each connection has a weight that is adjusted during learning. ANNs are used for a wide range of machine learning tasks, such as classification, regression, and pattern recognition.\n",
        "\n",
        "- **Structure**:\n",
        "  - **Input Layer**: Receives the input data.\n",
        "  - **Hidden Layers**: Layers between input and output that process the data through weighted connections.\n",
        "  - **Output Layer**: Produces the final output or prediction.\n",
        "\n",
        "### **What is Deep Learning?**\n",
        "\n",
        "**Deep Learning** is a subset of machine learning that utilizes deep neural networks, which are ANNs with many hidden layers (hence the term \"deep\"). Deep learning algorithms automatically learn hierarchical features from data, making them especially powerful for complex tasks like image and speech recognition, natural language processing, and game playing.\n",
        "\n",
        "- **Deep Networks**: Have multiple hidden layers that allow them to learn increasingly abstract representations of data.\n",
        "- **Learning**: Deep learning models learn directly from raw data, requiring minimal feature engineering.\n",
        "\n",
        "### **Why are ANN and Deep Learning Necessary?**\n",
        "\n",
        "1. **Handling Complex Data**\n",
        "2. **Automated Feature Extraction**\n",
        "3. **Improved Accuracy**\n",
        "4. **Scalability**\n",
        "5. **End-to-End Learning**\n",
        "6. **Advances in Hardware**"
      ],
      "metadata": {
        "id": "xtQ-2qtpGAIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizers\n",
        "\n",
        "Optimizers in deep learning are algorithms used to adjust the weights of the model in order to minimize the loss function. Here’s a breakdown of the most commonly used optimizers:\n",
        "\n",
        "#### **Stochastic Gradient Descent (SGD)**\n",
        "   - **How it works**: Updates the weights using a fraction of the training data (mini-batches) in each iteration.\n",
        "   - **Advantages**: Simple and effective for many problems, works well with large datasets.\n",
        "   - **Disadvantages**: It can get stuck in local minima, and often requires careful tuning of the learning rate.\n",
        "   \n",
        "   **Formula**:  \n",
        "   $w = w - \\eta \\cdot \\nabla L(w)$\n",
        "   where $ \\eta $ is the learning rate and $ \\nabla L(w) $ is the gradient of the loss function with respect to the weights.\n",
        "\n",
        "\n",
        "#### **Adam (Adaptive Moment Estimation)**\n",
        "   - **How it works**: Combines the advantages of both SGD with momentum and RMSprop (adaptive learning rate).\n",
        "   - **Advantages**: Adaptive learning rates for each parameter and typically requires less tuning.\n",
        "   - **Disadvantages**: Can sometimes lead to overfitting and may be sensitive to large batch sizes.\n",
        "\n",
        "   **Formula**:  \n",
        "   $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla L(w)$\n",
        "   $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla L(w))^2$\n",
        "   $w = w - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} \\cdot m_t$\n",
        "   where $ m_t $ is the first moment (mean) and $ v_t $ is the second moment (variance).\n",
        "\n",
        "\n",
        "#### 5. **RMSprop**\n",
        "   - **How it works**: Modifies Adagrad to help the learning rate from decaying too fast, especially for non-convex optimization problems.\n",
        "   - **Advantages**: Works well for a wide range of problems and does not suffer from rapid learning rate decay.\n",
        "   - **Disadvantages**: May still require fine-tuning of parameters.\n",
        "   \n",
        "   **Formula**:  \n",
        "   $v_t = \\beta v_{t-1} + (1 - \\beta) (\\nabla L(w))^2$\n",
        "   $w = w - \\frac{\\eta}{\\sqrt{v_t + \\epsilon}} \\cdot \\nabla L(w)$\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dg5GKnTdI1w7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Loss Functions\n",
        "\n",
        "Loss functions (or cost functions) quantify the difference between the predicted values and the actual values. Depending on the problem (classification or regression), different loss functions are used.\n",
        "\n",
        "#### For **Classification**:\n",
        "\n",
        "1. **Cross-Entropy Loss (Log Loss)**:\n",
        "   - **Used for**: Multi-class and binary classification tasks.\n",
        "   - **How it works**: Measures the difference between two probability distributions – the predicted probability distribution and the true distribution.\n",
        "   - **Formula**:\n",
        "     - For binary classification:\n",
        "       $\n",
        "       L(y, \\hat{y}) = -[y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y})]\n",
        "    $\n",
        "     - For multi-class classification:\n",
        "       $\n",
        "       L(y, \\hat{y}) = - \\sum_{i=1}^{C} y_i \\cdot \\log(\\hat{y}_i)\n",
        "    $\n",
        "     where $y_i$ is the true label and $\\hat{y}_i$ is the predicted probability for class $i$.\n",
        "\n",
        "2. **Sparse Categorical Cross-Entropy Loss**:\n",
        "   - **Used for**: Multi-class classification with integer labels (as opposed to one-hot encoding).\n",
        "   - **How it works**: Similar to cross-entropy loss but accepts integer labels.\n",
        "   \n",
        "3. **Binary Cross-Entropy (BCE)**:\n",
        "   - **Used for**: Binary classification.\n",
        "   - **Formula**:  \n",
        "     $\n",
        "     L(y, \\hat{y}) = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]\n",
        "  $\n",
        "     where $y$ is the actual class and $\\hat{y}$ is the predicted probability.\n",
        "\n",
        "#### For **Regression**:\n",
        "\n",
        "1. **Mean Squared Error (MSE) Loss**:\n",
        "   - **Used for**: Regression tasks where the output is a continuous value.\n",
        "   - **How it works**: Measures the average of the squared differences between predicted and actual values.\n",
        "   - **Formula**:\n",
        "     $\n",
        "     L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
        "  $\n",
        "     where $y_i$ is the true value and $\\hat{y}_i$ is the predicted value.\n",
        "\n",
        "2. **Mean Absolute Error (MAE) Loss**:\n",
        "   - **Used for**: Regression tasks, less sensitive to outliers than MSE.\n",
        "   - **How it works**: Measures the average of the absolute differences between predicted and actual values.\n",
        "   - **Formula**:\n",
        "     $\n",
        "     L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|\n",
        "  $\n",
        "\n",
        "3. **Huber Loss**:\n",
        "   - **Used for**: Regression tasks where you want to combine the benefits of MSE and MAE.\n",
        "   - **How it works**: Combines the best features of both MSE and MAE by using MSE when errors are small and MAE when errors are large.\n",
        "   - **Formula**:\n",
        "     $\n",
        "     L(y, \\hat{y}) =\n",
        "     \\begin{cases}\n",
        "     \\frac{1}{2} (y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta \\\\\n",
        "     \\delta |y - \\hat{y}| - \\frac{1}{2} \\delta^2 & \\text{otherwise}\n",
        "     \\end{cases}\n",
        "  $\n",
        "     where $ \\delta $ is a hyperparameter that controls the point at which the loss function transitions from quadratic to linear."
      ],
      "metadata": {
        "id": "9qoTKfRiJcAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When to Choose Each Optimizer and Loss Function\n",
        "\n",
        "1. **For Classification**:\n",
        "   - **Use Cross-Entropy Loss**: When you need a model to predict probabilities across multiple classes.\n",
        "   - **Optimizer**: Adam is commonly used as it combines the benefits of momentum and adaptive learning rates, making it robust for most tasks. SGD can work well too, but requires careful tuning of the learning rate.\n",
        "\n",
        "2. **For Regression**:\n",
        "   - **Use MSE or MAE Loss**: If the task involves predicting continuous values.\n",
        "   - **Optimizer**: Adam or RMSprop can be used for efficient convergence in regression tasks.\n",
        "\n",
        "3. **For Binary Classification**:\n",
        "   - **Use Binary Cross-Entropy Loss**: For binary classification tasks.\n",
        "   - **Optimizer**: Adam or SGD can be used effectively."
      ],
      "metadata": {
        "id": "GCoHRTr1Jdqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Functions\n",
        "\n",
        "Activation functions are mathematical functions applied to the output of a node (or neuron) in an artificial neural network. They introduce non-linearity into the model, which allows the network to learn complex patterns and make better predictions. Without activation functions, the neural network would be just a linear regression model, unable to solve non-linear problems.\n",
        "\n",
        "Here are the most commonly used activation functions in deep learning:\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. **Sigmoid (Logistic) Function**\n",
        "   - **Formula**:$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
        "   - **Range**: \\( (0, 1) \\)\n",
        "   - **Use Case**: Often used in the output layer for binary classification tasks (output probabilities).\n",
        "   - **Advantages**: Provides outputs that are easy to interpret as probabilities.\n",
        "   - **Disadvantages**: Can suffer from the **vanishing gradient problem** (gradients become very small for large input values), which can slow down training.\n",
        "  \n",
        "---\n",
        "\n",
        "#### 2. **Hyperbolic Tangent (Tanh)**\n",
        "   - **Formula**:\n",
        "     $\n",
        "     \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "     $\n",
        "   - **Range**: \\( (-1, 1) \\)\n",
        "   - **Use Case**: Often used in hidden layers, especially for tasks where outputs need to be centered around 0.\n",
        "   - **Advantages**: Unlike Sigmoid, Tanh is zero-centered, meaning the output has both negative and positive values, which helps reduce bias during training.\n",
        "   - **Disadvantages**: Like the Sigmoid function, Tanh can also suffer from the **vanishing gradient problem** for large values of \\( x \\).\n",
        "  \n",
        "---\n",
        "\n",
        "#### 3. **ReLU (Rectified Linear Unit)**\n",
        "   - **Formula**:\n",
        "     $\n",
        "     \\text{ReLU}(x) = \\max(0, x)\n",
        "     $\n",
        "   - **Range**: \\( [0, \\infty) \\)\n",
        "   - **Use Case**: Most commonly used activation function for hidden layers in deep networks.\n",
        "   - **Advantages**: Computationally efficient and less likely to suffer from the vanishing gradient problem compared to Sigmoid and Tanh.\n",
        "   - **Disadvantages**: Can suffer from the **dying ReLU problem**, where neurons can become inactive and always output zero if the weights are updated in a way that causes them to fall into a region where the derivative is zero.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Leaky ReLU**\n",
        "   - **Formula**:\n",
        "     $\n",
        "     \\text{Leaky ReLU}(x) = \\max(\\alpha x, x)\n",
        "     $\n",
        "     where \\( \\alpha \\) is a small constant (e.g., 0.01).\n",
        "   - **Range**: \\( (-\\infty, \\infty) \\)\n",
        "   - **Use Case**: Used to address the dying ReLU problem by allowing a small negative slope for \\( x < 0 \\).\n",
        "   - **Advantages**: Helps to prevent neurons from \"dying\" by allowing a small negative gradient when \\( x < 0 \\).\n",
        "   - **Disadvantages**: The choice of \\( \\alpha \\) is a hyperparameter that needs to be tuned.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **Parametric ReLU (PReLU)**\n",
        "   - **Formula**:\n",
        "     $\n",
        "     \\text{PReLU}(x) = \\max(\\alpha x, x)\n",
        "     $\n",
        "     where \\( \\alpha \\) is learned during training.\n",
        "   - **Range**: \\( (-\\infty, \\infty) \\)\n",
        "   - **Use Case**: A generalization of Leaky ReLU where the negative slope \\( \\alpha \\) is a parameter that can be learned during training.\n",
        "   - **Advantages**: More flexible than Leaky ReLU since \\( \\alpha \\) is not fixed and can be optimized during training.\n",
        "   - **Disadvantages**: Increased model complexity due to learning an additional parameter.\n",
        "\n",
        "---\n",
        "\n",
        "#### 6. **Softmax**\n",
        "   - **Formula**:\n",
        "     $\n",
        "     \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
        "     $\n",
        "     where \\( x_i \\) is the input to the \\( i \\)-th neuron, and the denominator is the sum of exponentials of all inputs.\n",
        "   - **Range**: \\( (0, 1) \\) for each output, and the sum of all outputs equals 1.\n",
        "   - **Use Case**: Used in the output layer for multi-class classification problems.\n",
        "   - **Advantages**: Converts raw scores into probabilities, making it suitable for classification tasks with multiple classes.\n",
        "   - **Disadvantages**: Can be computationally expensive for very large datasets, as it requires calculating the exponentials of all inputs.\n",
        "\n",
        "---\n",
        "\n",
        "#### 7. **Swish**\n",
        "   - **Formula**:\n",
        "     $\n",
        "     \\text{Swish}(x) = x \\cdot \\sigma(x)\n",
        "     $\n",
        "     where \\( \\sigma(x) \\) is the Sigmoid function.\n",
        "   - **Range**: \\( (-\\infty, \\infty) \\)\n",
        "   - **Use Case**: Used in deep learning models, especially for very deep networks.\n",
        "   - **Advantages**: It is a smooth, non-monotonic function that tends to perform better than ReLU in some cases.\n",
        "   - **Disadvantages**: More computationally expensive than ReLU.\n",
        "\n",
        "---\n",
        "\n",
        "#### 8. **Softplus**\n",
        "   - **Formula**:\n",
        "     $\n",
        "     \\text{Softplus}(x) = \\log(1 + e^x)\n",
        "     $\n",
        "   - **Range**: \\( (0, \\infty) \\)\n",
        "   - **Use Case**: A smooth approximation of ReLU.\n",
        "   - **Advantages**: Avoids the sharp cutoff that ReLU introduces and can be differentiable everywhere.\n",
        "   - **Disadvantages**: Computationally more expensive than ReLU.\n",
        "\n",
        "---\n",
        "\n",
        "### Choosing the Right Activation Function\n",
        "\n",
        "- **For Hidden Layers**:\n",
        "  - **ReLU** is the most widely used due to its simplicity and effectiveness, especially when used with large networks.\n",
        "  - **Leaky ReLU** or **PReLU** is often chosen when ReLU's \"dying neuron\" issue is a concern.\n",
        "  - **Tanh** can be used when the outputs should be zero-centered.\n",
        "\n",
        "- **For Output Layer**:\n",
        "  - **Sigmoid**: Best for binary classification when the output needs to be a probability.\n",
        "  - **Softmax**: Used for multi-class classification problems where the output represents probabilities of each class.\n",
        "  - **Linear**: Used for regression tasks where the output can be any real number.\n",
        "\n",
        "### Summary\n",
        "- **ReLU** is the default for hidden layers, and **Sigmoid** or **Softmax** is typically used for output layers in classification tasks.\n",
        "- Activation functions help the neural network learn complex patterns and allow for the non-linear transformation of data, which is crucial for tasks like classification and regression.\n"
      ],
      "metadata": {
        "id": "XCuKhIk-Jx9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: Classification (Spaceship Titaic)"
      ],
      "metadata": {
        "id": "_RyGqIVLI0wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, average_precision_score"
      ],
      "metadata": {
        "id": "pOvm6q3QC0pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "hA3Z_An7Gtc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X, y are in the form of pandas DataFrame/Series or numpy arrays\n",
        "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoader for batching\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "hkvz66OJL7Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape[1]"
      ],
      "metadata": {
        "id": "wJRr-PJU0SZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple ANN model\n",
        "class ANN_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANN_Model, self).__init__()\n",
        "        self.layer1 = nn.Linear(X.shape[1], 32)\n",
        "        self.layer2 = nn.Linear(32, 16)\n",
        "        self.output = nn.Linear(16, 2)  # For binary classification (0, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = torch.relu(self.layer2(x))\n",
        "        return self.output(x)"
      ],
      "metadata": {
        "id": "xgy5PVacMCWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "model = ANN_Model().to(device)\n",
        "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class, works with binary classification too\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    # Training loop with tqdm\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=100):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Collect the true labels and predictions\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Calculate training metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"Train Loss: {running_loss / len(train_loader):.4f}, Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}, Recall: {recall:.4f}, Precision: {precision:.4f}\")\n"
      ],
      "metadata": {
        "id": "l5ObhrCSL7IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2: Regression (FoodCourt)"
      ],
      "metadata": {
        "id": "krwFRZxbMe7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "kCyVVT5CNMzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_df.drop(['Transported','FoodCourt'], axis =1 )\n",
        "y = train_df['FoodCourt']"
      ],
      "metadata": {
        "id": "4WplqXGDPD2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_df, y_df are pandas DataFrames/Series\n",
        "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)  # For regression, reshape y"
      ],
      "metadata": {
        "id": "t0dAK1RGNmvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoader for batching\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "thOrkPXINRNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple ANN model for regression\n",
        "class ANN_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANN_Model, self).__init__()\n",
        "        self.layer1 = nn.Linear(X_train.shape[1], 32)\n",
        "        self.layer2 = nn.Linear(32, 16)\n",
        "        self.output = nn.Linear(16, 1)  # For regression output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = torch.relu(self.layer2(x))\n",
        "        return self.output(x)"
      ],
      "metadata": {
        "id": "E9hhw_kZPegP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "model = ANN_Model().to(device)\n",
        "criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "bCUsuVg2Pf-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    # Training loop with tqdm\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=100):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Collect the true labels and predictions\n",
        "        running_loss += loss.item()\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(outputs.cpu().detach().numpy())\n",
        "\n",
        "    # Calculate training metrics\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"Train Loss: {running_loss / len(train_loader):.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(outputs.cpu().detach().numpy())\n",
        "\n",
        "    # Calculate validation metrics\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"Validation MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")"
      ],
      "metadata": {
        "id": "ZrtiRu9QPju2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}